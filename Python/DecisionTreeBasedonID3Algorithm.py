# -*- coding: utf-8 -*-
"""Decision Tree Based on ID3 Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xld6fh-9UInuAM7Gxmx0ZtxyqgiJjOBW
"""

IMPLEMENTATION OF DECISION TREE BASED ON ID3 ALGORITHM:
Decision tree is the most powerful and popular tool for classification and prediction. 
A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.

Construction of Decision Tree : 
A tree can be “learned” by splitting the source set into subsets based on an attribute value test. 
This process is repeated on each derived subset in a recursive manner called recursive partitioning. 
The recursion is completed when the subset at a node all has the same value of the target variable, or when splitting no longer adds value to the predictions. 
The construction of decision tree classifier does not require any domain knowledge or parameter setting, and therefore is appropriate for exploratory knowledge discovery. 
Decision trees can handle high dimensional data. In general decision tree classifier has good accuracy. 
Decision tree induction is a typical inductive approach to learn knowledge on classification.

Decision Tree Representation: 
Decision trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification of the instance. 
An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute as shown in the above figure. 
This process is then repeated for the subtree rooted at the new node.

In Decision Tree the major challenge is to identification of the attribute for the root node in each level. 
This process is known as attribute selection. We have two popular attribute selection measures:
1. Information Gain
2. Gini Index

1. Information Gain
When we use a node in a decision tree to partition the training instances into smaller subsets the entropy changes. 
Information gain is a measure of this change in entropy.

(i) Entropy
Entropy is the measure of uncertainty of a random variable, it characterizes the impurity of an arbitrary collection of examples. 
The higher the entropy more the information content.

2. Gini Index
Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.
It means an attribute with lower Gini index should be preferred.

The most notable types of decision tree algorithms are:-
1. Iterative Dichotomiser 3 (ID3)
2. C4.5
3. Classification and Regression Tree(CART)

Iterative Dichotomiser 3 (ID3): 
This algorithm uses Information Gain to decide which attribute is to be used classify the current subset of the data. 
For each level of the tree, information gain is calculated for the remaining data recursively.

Importing the required libraries:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
eps = np.finfo(float).eps
from numpy import log2 as log

‘eps’ here is the smallest representable number. At times we get log(0) or 0 in the denominator, to avoid that we are going to use this.

Reading and defining the datatset:

from google.colab import files
uploaded = files.upload()

import io
data = pd.read_csv(io.BytesIO(uploaded['play_tennis.csv']))
# Dataset is now stored in a Pandas Dataframe

print(data)

Creating pandas dataframe:

data = data.drop("day", axis=1)
df = pd.DataFrame(data, columns=data.keys())

Steps for creating a decision tree:
1. Compute the entropy for data-set
2. For every attribute/feature:
       i.) Calculate entropy for all categorical values
       ii.) Take average information entropy for the current attribute
       iii.) Calculate gain for the current attribute
3. Pick the highest gain attribute.
4. Repeat until we get the tree we desire.

# 1. Calculating entropy of the whole dataset

entropy_node = 0  # Initialize Entropy
values = df.play.unique()  # Unique objects - 'Yes', 'No'
for value in values:
    fraction = df.play.value_counts()[value]/len(df.play)  
    entropy_node += -fraction*np.log2(fraction)

# 2. Now defining a function {ent} to calculate entropy of each attribute 
def ent(df,attribute):
    target_variables = df.play.unique()  # This gives all 'Yes' and 'No'
    variables = df[attribute].unique()    # This gives different features in that attribute 


    entropy_attribute = 0
    for variable in variables:
        entropy_each_feature = 0
        for target_variable in target_variables:
            num = len(df[attribute][df[attribute]==variable][df.play ==target_variable]) # numerator
            den = len(df[attribute][df[attribute]==variable])  # denominator
            fraction = num/(den+eps)  # pi
            entropy_each_feature += -fraction*log(fraction+eps) # This calculates entropy for one feature 
        fraction2 = den/len(df)
        entropy_attribute += -fraction2*entropy_each_feature   # Sums up all the entropy 

    return(abs(entropy_attribute))

# Storing entropy of each attribute with its name
a_entropy = {k:ent(df,k) for k in df.keys()[:-1]}
a_entropy

# 3. Calculating information gain of each attribute
def ig(e_dataset,e_attr):
    return(e_dataset-e_attr)

# Storing information gain of each attribute in a dictionary

# entropy_node = entropy of dataset
# a_entropy[k] = entropy of k(th) attr
IG = {k:ig(entropy_node,a_entropy[k]) for k in a_entropy}

As we can see outlook has the highest info gain of 0.24, therefore we will select outook as the node at this level for splitting.

To proceed with our tree we will use recursion and we will build a tree based on this:

def find_entropy(df):
    Class = df.keys()[-1]   # To make the code generic, changing target variable class name
    entropy = 0
    values = df[Class].unique()
    for value in values:
        fraction = df[Class].value_counts()[value]/len(df[Class])
        entropy += -fraction*np.log2(fraction)
    return entropy
  
  
def find_entropy_attribute(df,attribute):
  Class = df.keys()[-1]   # To make the code generic, changing target variable class name
  target_variables = df[Class].unique()  # This gives all 'Yes' and 'No'
  variables = df[attribute].unique()    # This gives different features in that attribute (like 'Hot','Cold' in Temperature)
  entropy2 = 0
  for variable in variables:
      entropy = 0
      for target_variable in target_variables:
          num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])
          den = len(df[attribute][df[attribute]==variable])
          fraction = num/(den+eps)
          entropy += -fraction*log(fraction+eps)
      fraction2 = den/len(df)
      entropy2 += -fraction2*entropy
  return abs(entropy2)


def find_winner(df):
    Entropy_att = []
    IG = []
    for key in df.keys()[:-1]:
        # Entropy_att.append(find_entropy_attribute(df,key))
        IG.append(find_entropy(df)-find_entropy_attribute(df,key))
    return df.keys()[:-1][np.argmax(IG)]
  
  
def get_subtable(df, node,value):
  return df[df[node] == value].reset_index(drop=True)


def buildTree(df,tree=None): 
    Class = df.keys()[-1]   # To make the code generic, changing target variable class name
    
    # Here we build our decision tree

    # Get attribute with maximum information gain
    node = find_winner(df)
    
    # Get distinct value of that attribute e.g Salary is node and Low, Med and High are values
    attValue = np.unique(df[node])
    
    # Create an empty dictionary to create tree    
    if tree is None:                    
        tree={}
        tree[node] = {}
    
   # We make loop to construct a tree by calling this function recursively. 
    # In this we check if the subset is pure and stop if it is pure. 

    for value in attValue:
        
        subtable = get_subtable(df,node,value)
        clValue,counts = np.unique(subtable[df.columns[-1]],return_counts=True)                        
        
        if len(counts)==1:#Checking purity of subset
            tree[node][value] = clValue[0]                                                    
        else:        
            tree[node][value] = buildTree(subtable) #Calling the function recursively 
                   
    return tree

t = buildTree(df)

import pprint
pprint.pprint(t)

Visualizing the decision tree using Scikit-learn library:

from sklearn.preprocessing import LabelEncoder
Le = LabelEncoder()

data['outlook'] = Le.fit_transform(data['outlook'])
data['temp'] = Le.fit_transform(data['temp'])
data['humidity'] = Le.fit_transform(data['humidity'])
data['wind'] = Le.fit_transform(data['wind'])
data['play'] = Le.fit_transform(data['play'])

y = data['play']
X = data.drop(['play'],axis=1)

from sklearn import tree
clf = tree.DecisionTreeClassifier(criterion = 'entropy')
clf = clf.fit(X, y)

# We can visualize the tree using tree.plot_tree
tree.plot_tree(clf)

import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph
